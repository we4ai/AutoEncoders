{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB to GrayScale AutoEncoder\n",
    "In this project, we are going to test the power of AutoEncoder for converting the RGB images to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlowerAutoEncoder(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (t_conv1): ConvTranspose2d(32, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (t_conv2): ConvTranspose2d(64, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (t_conv3): ConvTranspose2d(16, 1, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FlowerAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlowerAutoEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(64, 32, 3, padding = 1)\n",
    "        ## 128 -> 64 -> 32 -> 16 \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.t_conv1 = nn.ConvTranspose2d(32, 64, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(64, 16, 2, stride=2)\n",
    "        self.t_conv3 = nn.ConvTranspose2d(16, 1, 2, stride=2)\n",
    "        ## 16 -> 32 -> 64 -> 128\n",
    "        \n",
    "    def forward(self, x):\n",
    "#        x.view(x.size(0))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x) #128 -> 64\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x) #64 -> 32\n",
    "        x = F.relu(self.conv3(x)) ## 32 -> 32\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)   #32 -> 16\n",
    "        \n",
    "        \n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.relu(self.t_conv2(x))\n",
    "        x = F.sigmoid(self.t_conv3(x))\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = FlowerAutoEncoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_transform = transforms.Compose([ \n",
    "                               transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "                            \n",
    "                               ])\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                               ])\n",
    "\n",
    "\n",
    "color_imgs = datasets.ImageFolder('new_color_images/', transform= transform)\n",
    "gray_imgs  = datasets.ImageFolder('new_gray_images/', transform= gray_transform)\n",
    "color_train_loader = DataLoader(dataset=color_imgs, batch_size=32, shuffle=False, num_workers=0)\n",
    "gray_train_loader  = DataLoader(dataset=gray_imgs,  batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 101 train_loss : 164.4949079155922\n",
      "i: 102 train_loss : 164.870320469141\n",
      "i: 103 train_loss : 165.39048072695732\n",
      "i: 104 train_loss : 165.77197855710983\n",
      "i: 105 train_loss : 166.2068169414997\n",
      "i: 106 train_loss : 166.69465002417564\n",
      "i: 107 train_loss : 167.20185247063637\n",
      "i: 108 train_loss : 167.6479260623455\n",
      "i: 109 train_loss : 168.03916016221046\n",
      "i: 110 train_loss : 168.54111739993095\n",
      "i: 111 train_loss : 168.9396480023861\n",
      "i: 112 train_loss : 169.4237543642521\n",
      "i: 113 train_loss : 169.9024589061737\n",
      "Final Loss:  1.4903724465453834\n"
     ]
    }
   ],
   "source": [
    "train_loss =0.0\n",
    "for i, data in enumerate(zip(color_train_loader, gray_train_loader)):\n",
    "    color_imgs, _ = data[0]\n",
    "    gray_imgs, _  = data[1]\n",
    "    \n",
    "    optimizer.zero_grad()                #Z\n",
    "    outputs = model(color_imgs)           #M\n",
    "    \n",
    "    loss = criterion(outputs, gray_imgs) #C\n",
    "    loss.backward()                      #B\n",
    "    optimizer.step()                     #S\n",
    "    train_loss += loss.item()*len(color_imgs)\n",
    "    if i >100 :\n",
    "        print(\"i:\", i, \"train_loss :\", train_loss)\n",
    "print(\"Final Loss: \",train_loss/len(color_train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss:  0.4615851951795712\n",
      "Final Loss:  0.37972524788296014\n",
      "Final Loss:  0.349574395569793\n",
      "Final Loss:  0.32863062973085205\n",
      "Final Loss:  0.3141281589081413\n",
      "Final Loss:  0.3033866963365622\n",
      "Final Loss:  0.29555684458791165\n",
      "Final Loss:  0.2877801646266067\n",
      "Final Loss:  0.2824120001311888\n",
      "Final Loss:  0.27786807689750403\n",
      "Final Loss:  0.2716353173580086\n",
      "Final Loss:  0.27110359595533\n",
      "Final Loss:  0.2642918461770342\n",
      "Final Loss:  0.2617973895710811\n",
      "Final Loss:  0.2580045415905484\n",
      "Final Loss:  0.253218232801086\n",
      "Final Loss:  0.24965701056154152\n",
      "Final Loss:  0.2487724896585732\n",
      "Final Loss:  0.24483934698397652\n",
      "Final Loss:  0.24210377240128683\n",
      "Final Loss:  0.2397237484130943\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 20\n",
    "for epoch in range(total_epochs +1):\n",
    "    train_loss =0.0\n",
    "    for i, data in enumerate(zip(color_train_loader, gray_train_loader)):\n",
    "        color_imgs, _ = data[0]\n",
    "        gray_imgs, _  = data[1]\n",
    "\n",
    "        optimizer.zero_grad()                #Z\n",
    "        outputs = model(color_imgs)           #M\n",
    "        loss = criterion(outputs, gray_imgs) #C\n",
    "        loss.backward()                      #B\n",
    "        optimizer.step()                     #S\n",
    "        train_loss += loss.item()*len(color_imgs)\n",
    "        \n",
    "    print(\"Final Loss: \",train_loss/len(color_train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
